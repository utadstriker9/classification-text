{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.text_preprocessing import generate_preprocessor\n",
    "import src.utils as utils\n",
    "from src.model import train_model, get_best_model, get_best_threshold\n",
    "\n",
    "\n",
    "# Read Data\n",
    "def read_data(return_file=True):\n",
    "    existing_data = utils.load_json(CONFIG_DATA[\"data_set_path\"])\n",
    "    existing_data = existing_data.drop_duplicates(\n",
    "        subset=CONFIG_DATA[\"text_column\"], keep=\"first\"\n",
    "    )\n",
    "    print(\"Existing data inputted, data shape  :\", existing_data.shape)\n",
    "    new_data = pd.read_excel(CONFIG_DATA[\"raw_new_dataset_path\"])\n",
    "\n",
    "    # print data\n",
    "    print(\"new data inputted, data shape  :\", new_data.shape)\n",
    "\n",
    "    # Remove duplicates data\n",
    "    new_data = new_data.drop_duplicates(subset=CONFIG_DATA[\"text_column\"], keep=\"first\")\n",
    "    new_data_exc = new_data[\n",
    "        ~new_data[CONFIG_DATA[\"text_column\"]].isin(\n",
    "            existing_data[CONFIG_DATA[\"text_column\"]]\n",
    "        )\n",
    "    ]\n",
    "    data = pd.concat([existing_data, new_data_exc], axis=0, ignore_index=True)\n",
    "\n",
    "    # Print data\n",
    "    print(\"ready read file, data shape   :\", new_data_exc.shape)\n",
    "\n",
    "    # Return data\n",
    "    if return_file:\n",
    "        return {\"new_data\": new_data_exc, \"data\": data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\taqiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taqiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import HTMLResponse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "import src.utils as utils\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "CONFIG_DATA = utils.config_load()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Load your pre-trained ML model\n",
    "class Model:\n",
    "    def clean_data(self, data, CONFIG_DATA=CONFIG_DATA, return_file=True):\n",
    "        # print(\"cleaning the data\")\n",
    "\n",
    "        # Lowering Case\n",
    "        data[CONFIG_DATA[\"text_column\"]] = data[CONFIG_DATA[\"text_column\"]].str.lower()\n",
    "\n",
    "        # Remove Non ASCII\n",
    "        data[CONFIG_DATA[\"text_column\"]] = (\n",
    "            data[CONFIG_DATA[\"text_column\"]]\n",
    "            .str.encode(\"ascii\", \"ignore\")\n",
    "            .str.decode(\"ascii\")\n",
    "        )\n",
    "\n",
    "        # Remove Whitespace in Start and End\n",
    "        data[CONFIG_DATA[\"text_column\"]] = data[CONFIG_DATA[\"text_column\"]].str.strip()\n",
    "\n",
    "        # Punctuation Removal Code\n",
    "        data[CONFIG_DATA[\"text_column\"]] = data[CONFIG_DATA[\"text_column\"]].apply(\n",
    "            lambda text: re.sub(\n",
    "                r\"[{}]\".format(re.escape(string.punctuation)), \"\", str(text)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Remove Mention, Link, Hashtag, etc\n",
    "        def replace_str(text):\n",
    "            cleaned_text = re.sub(\n",
    "                r\"[\\n\\r\\t]\", \" \", str(text)\n",
    "            )  # Remove Tab, Enter, Space\n",
    "            cleaned_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", str(text))  # Remove Many Hwer\n",
    "            cleaned_text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(text))  # Remove 1 Char Only\n",
    "            cleaned_text = re.sub(r\"\\d+\", \"\", str(text))  # Remove Number\n",
    "            return cleaned_text\n",
    "\n",
    "        data[CONFIG_DATA[\"text_column\"]] = data[CONFIG_DATA[\"text_column\"]].apply(\n",
    "            replace_str\n",
    "        )\n",
    "\n",
    "        # Remove incomplete URL\n",
    "        data[CONFIG_DATA[\"text_column\"]] = (\n",
    "            data[CONFIG_DATA[\"text_column\"]]\n",
    "            .replace(\"http://\", \" \")\n",
    "            .replace(\"https://\", \" \")\n",
    "        )\n",
    "\n",
    "        # Remove Multiple Space\n",
    "        data[CONFIG_DATA[\"text_column\"]] = data[CONFIG_DATA[\"text_column\"]].replace(\n",
    "            r\"\\s+\", \" \", regex=True\n",
    "        )\n",
    "\n",
    "        # print(\"data was cleaned\")\n",
    "        if return_file:\n",
    "            return data\n",
    "\n",
    "    # Generate Preprocessor\n",
    "    def generate_preprocessor(self, data, CONFIG_DATA=CONFIG_DATA, return_file=True):\n",
    "        # clean data\n",
    "        # print(\"ready to clean data\")\n",
    "        data = self.clean_data(data, CONFIG_DATA)\n",
    "\n",
    "        # print(\"ready to preprocess\")\n",
    "\n",
    "        # tokenization\n",
    "        def word_tokenize_wrapper(text):\n",
    "            return word_tokenize(text)\n",
    "\n",
    "        data[CONFIG_DATA[\"token_column\"]] = data[CONFIG_DATA[\"text_column\"]].apply(\n",
    "            word_tokenize_wrapper\n",
    "        )\n",
    "\n",
    "        # Stopwords Removal (Filtering)\n",
    "        list_stopwords = stopwords.words(\"indonesian\")\n",
    "        list_stopwords.extend(\n",
    "            [\n",
    "                \"yg\",\n",
    "                \"dg\",\n",
    "                \"rt\",\n",
    "                \"dgn\",\n",
    "                \"ny\",\n",
    "                \"d\",\n",
    "                \"klo\",\n",
    "                \"kalo\",\n",
    "                \"amp\",\n",
    "                \"biar\",\n",
    "                \"bikin\",\n",
    "                \"bilang\",\n",
    "                \"gak\",\n",
    "                \"ga\",\n",
    "                \"krn\",\n",
    "                \"nya\",\n",
    "                \"nih\",\n",
    "                \"sih\",\n",
    "                \"si\",\n",
    "                \"tau\",\n",
    "                \"tdk\",\n",
    "                \"tuh\",\n",
    "                \"utk\",\n",
    "                \"ya\",\n",
    "                \"jd\",\n",
    "                \"jgn\",\n",
    "                \"sdh\",\n",
    "                \"aja\",\n",
    "                \"n\",\n",
    "                \"t\",\n",
    "                \"nyg\",\n",
    "                \"hehe\",\n",
    "                \"pen\",\n",
    "                \"u\",\n",
    "                \"nan\",\n",
    "                \"loh\",\n",
    "                \"rt\",\n",
    "                \"&amp\",\n",
    "                \"yah\",\n",
    "            ]\n",
    "        )\n",
    "        list_stopwords = set(list_stopwords)\n",
    "\n",
    "        def stopwords_removal(words):\n",
    "            return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "        # print(\"gagal stopwords\")\n",
    "\n",
    "        data[CONFIG_DATA[\"token_column\"]] = data[CONFIG_DATA[\"token_column\"]].apply(\n",
    "            stopwords_removal\n",
    "        )\n",
    "        # Stemming\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "\n",
    "        def stemmed_wrapper(term):\n",
    "            return stemmer.stem(term)\n",
    "\n",
    "        term_dict = {}\n",
    "\n",
    "        for document in data[CONFIG_DATA[\"token_column\"]]:\n",
    "            for term in document:\n",
    "                if term not in term_dict:\n",
    "                    term_dict[term] = \" \"\n",
    "\n",
    "        for term in term_dict:\n",
    "            term_dict[term] = stemmed_wrapper(term)\n",
    "\n",
    "        def get_stemmed_term(document):\n",
    "            return [term_dict[term] for term in document]\n",
    "\n",
    "        # print(\"gagal stemming\")\n",
    "\n",
    "        data[CONFIG_DATA[\"token_column\"]] = data[\n",
    "            CONFIG_DATA[\"token_column\"]\n",
    "        ].swifter.apply(get_stemmed_term)\n",
    "\n",
    "        # print('data was processed')\n",
    "        if return_file:\n",
    "            return data\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Function to predict the data\"\"\"\n",
    "        # Preprocess data\n",
    "        # print(\"ready to go\")\n",
    "        X_clean = self.generate_preprocessor(X)  # Use generate_preprocessor\n",
    "        # Convert tokenized texts to string format\n",
    "        # print(\"ready to run model\")\n",
    "        X_clean = [\" \".join(tokens) for tokens in X_clean[CONFIG_DATA[\"token_column\"]]]\n",
    "        # Create TF-IDF vectorizer\n",
    "        tfidf_vectorizer = utils.pickle_load(CONFIG_DATA[\"tfidf_vectorizer_path\"])\n",
    "        # Transform the data into TF-IDF features\n",
    "        # print(\"ready to tfidf\")\n",
    "        X_clean = tfidf_vectorizer.transform(X_clean)\n",
    "        # Predict data\n",
    "        # print(\"ready to predict\")\n",
    "        model = utils.pickle_load(CONFIG_DATA[\"best_model_path\"])\n",
    "        y_pred = model.predict_proba(X_clean)\n",
    "\n",
    "        # Create Label Encoder\n",
    "        label_encoder = utils.pickle_load(CONFIG_DATA[\"label_encoder_path\"])\n",
    "\n",
    "        # Transform class indices to original class labels\n",
    "        class_labels = label_encoder.inverse_transform(\n",
    "            np.arange(len(label_encoder.classes_))\n",
    "        )\n",
    "\n",
    "        # Create a list to store class labels with probabilities\n",
    "        class_prob_list = []\n",
    "\n",
    "        # Iterate through each input sample and append class label with probability to the list\n",
    "        for row in y_pred:\n",
    "            class_probabilities = [\n",
    "                {\"label\": label, \"probability\": prob}\n",
    "                for label, prob in zip(class_labels, row)\n",
    "            ]\n",
    "            class_prob_list.append(class_probabilities)\n",
    "\n",
    "        # Return the list of dictionaries containing class labels and probabilities\n",
    "        return class_prob_list\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\n",
    "def predict_text(data):\n",
    "    try:\n",
    "        text = data\n",
    "        if not text:\n",
    "            raise HTTPException(\n",
    "                status_code=400, detail=\"Missing 'text' field in request payload\"\n",
    "            )\n",
    "\n",
    "        # Preprocess data\n",
    "        input_data = pd.DataFrame({\"content\": [text]})\n",
    "        predictions = model.predict(input_data)  # Use the model to make predictions\n",
    "\n",
    "        # Return predictions for each class\n",
    "        return {\"class_probabilities\": predictions}\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7549bbf2e9be460eb5d11f5126c0f9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'class_probabilities': [[{'label': 'ekonomi', 'probability': 0.18364586},\n",
       "   {'label': 'hukum', 'probability': 0.14363588},\n",
       "   {'label': 'ideologi', 'probability': 0.06176976},\n",
       "   {'label': 'pertahanan militer', 'probability': 0.1306013},\n",
       "   {'label': 'politik', 'probability': 0.29653746},\n",
       "   {'label': 'sosial budaya', 'probability': 0.18380976}]]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Warga yang tinggal di jalan Taucit, Kelurahan Labuhan Deli, Kecamatan Medan Marelan kini tidak lagi mengeluhkan kondisi akses jalan yang rusak, sebab Pemko Medan melalui Dinas SDABMBK telah selesai memperbaiki jalan dengan pembetonan di jalan tersebut, Kamis (7/9/2023) Dengan selesainya perbaikan jalan ini tentu pengguna jalan terutama masyarakat yang tinggal di jalan tersebut juga akan lebih merasakan kenyamanan. Karena selama ini kondisi jalan berbatu dan berlubang, sehingga jika hujan turun jalan menjadi becek dan berlumpur yang sedikit menyulitkan pengendara kendaraan. Perbaikan dengan pembetonan di jalan Taucit ini dilakukan Dinas SDABMBK dengan panjang 307 meter dengan lebar 5- 6 meter dan ketebalan beton 25 cm serta Beton FC 30 Mpa. Pembetonan jalan ini dilakukan selain adanya permintaan dari warga yang mengeluhkan kondisi jalan, juga sebagai upaya dalam mewujudkan program prioritas Wali Kota Medan Bobby Nasution yakni Medan Tanpa Lubang. Kepala Dinas SDABMBK Kota Medan Topan Obaja Putra Ginting menjelaskan pihaknya setiap harinya terus menggenjot memperbaiki sejumlah ruas jalan. Seperti yang telah dilakukan di jalan Taucit yang kondisi jalan sebelumnya tidak nyaman dilalui oleh pengendara, kini jalan sudah dibeton. Dulunya jalan Taucit ini berlubang dan bebatuan, kini sebagai upaya mewujudkan program prioritas pak Bobby Nasution, kami lakukan perbaikan dengan pembetonan agar lebih kuat dan tidak menggunakan kenyamanan masyarakat, jelasnya.\"\n",
    "\n",
    "ak = predict_text(text)\n",
    "ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "politik               2533\n",
       "ekonomi               1946\n",
       "ideologi              1640\n",
       "pertahanan militer    1616\n",
       "hukum                 1610\n",
       "sosial budaya         1469\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.text_preprocessing import generate_preprocessor\n",
    "import src.utils as utils\n",
    "from src.model import train_model, get_best_model, get_best_threshold\n",
    "\n",
    "CONFIG_DATA = utils.config_load()\n",
    "\n",
    "sasa = utils.load_json(CONFIG_DATA['data_clean_path'])\n",
    "sasa['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model_record = pd.DataFrame(\n",
    "        {\n",
    "            \"timestamp\": [utils.time_stamp()],\n",
    "            \"model_name\": [\"model_name\"],\n",
    "            \"model_params\": [\"model_params\"],\n",
    "            \"threshold\": [\"best_threshold\"],\n",
    "            \"metric_score\": [2],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_params</th>\n",
       "      <th>threshold</th>\n",
       "      <th>metric_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-27 18:40:34.786738</td>\n",
       "      <td>model_name</td>\n",
       "      <td>model_params</td>\n",
       "      <td>best_threshold</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp  model_name  model_params       threshold  \\\n",
       "0 2023-08-27 18:40:34.786738  model_name  model_params  best_threshold   \n",
       "\n",
       "   metric_score  \n",
       "0             2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_params</th>\n",
       "      <th>threshold</th>\n",
       "      <th>metric_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-27 05:30:00</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 100, 'ra...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp              model_name  \\\n",
       "0 2023-08-27 05:30:00  RandomForestClassifier   \n",
       "\n",
       "                                        model_params  threshold  metric_score  \n",
       "0  {'criterion': 'gini', 'n_estimators': 100, 'ra...        0.1           0.2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dada = utils.load_json(CONFIG_DATA['model_record'])\n",
    "dada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_params</th>\n",
       "      <th>threshold</th>\n",
       "      <th>metric_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-27 05:30:00.000000</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 100, 'ra...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-27 18:40:34.786738</td>\n",
       "      <td>model_name</td>\n",
       "      <td>model_params</td>\n",
       "      <td>best_threshold</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp              model_name  \\\n",
       "0 2023-08-27 05:30:00.000000  RandomForestClassifier   \n",
       "1 2023-08-27 18:40:34.786738              model_name   \n",
       "\n",
       "                                        model_params       threshold  \\\n",
       "0  {'criterion': 'gini', 'n_estimators': 100, 'ra...             0.1   \n",
       "1                                       model_params  best_threshold   \n",
       "\n",
       "   metric_score  \n",
       "0           0.2  \n",
       "1           2.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dada = pd.concat([dada, model_record], axis=0, ignore_index=True)\n",
    "dada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read Data\n",
    "def read_data(return_file=False):\n",
    "    existing_data = utils.load_json(CONFIG_DATA['data_set_path'])\n",
    "    new_data = pd.read_excel(CONFIG_DATA['raw_new_dataset_path'])\n",
    "    new_data_exc = new_data[~new_data[CONFIG_DATA['text_column']].isin(existing_data[CONFIG_DATA['text_column']])]\n",
    "    data = pd.concat([existing_data, new_data_exc], axis=0, ignore_index=True)\n",
    "\n",
    "    # Print data\n",
    "    print('ready read file, data shape   :', new_data_exc.shape)\n",
    "\n",
    "    # Dump data\n",
    "    utils.dump_json(data, CONFIG_DATA['data_set_path'])\n",
    "    utils.dump_json(new_data_exc, CONFIG_DATA['new_data_set_path'])\n",
    "    # Return data\n",
    "    if return_file:\n",
    "        return new_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
